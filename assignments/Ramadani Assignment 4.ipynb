{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration, Cleaning, and Analysis Guide\n",
    "\n",
    "## Step One: Import necessary modules\n",
    "\n",
    "NumPy will allow access to arrays, basic mathematics and statistical concepts, and other useful tools. pandas allows you to create DataFrames, and modify/add/update/remove columns and rows within those DataFrames. \n",
    "\n",
    "`import numpy as np\n",
    "import pandas as pd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Two: Read in a data source\n",
    "\n",
    "First, determine what kind of file type you have. You can tell by the file extension (for example, \"dataset.csv\" is a csv file). There are several types of file extensions that are used for data analysis, and different types are appropriate for different kinds of analysis. The two most common types are CSV, or Comma Separated Values, and JSON, or JavaScript Object Notation (e.g. \"dataset.json\"). The other types are: Excel (.xlsx), XML (.xml), UTF-8 (usually .txt), and key-value dictionaries (these can actually be created within a Jupyter notebook).\n",
    "\n",
    "The hardest part of reading in a data source is figuring out what the path to the file is from your notebook, and will likely be the most common error you experience at this stage. Make sure that you know where your data source is located, and then where your notebook is located. As always, your best bet is to start from the root path (usually \"/Users/...\" on a Mac).\n",
    "\n",
    "Next, name your dataframe loading variable. Many people like to name their dataframes \"df\", and you will see the variable used frequently while doing research, but you can name the dataframe anything you want. We'll be using \"df\" to get you used to the \"standard\" in this notebook.\n",
    "\n",
    "The data source we will be using is [expenditures from the U.S. House of Representatives](https://projects.propublica.org/represent/expenditures).\n",
    "\n",
    "`df = pd.read_csv(\"2019Q3-house-disburse-detail.csv\")`\n",
    "\n",
    "When reading in a data source, you may also run across encoding issues - Sometimes a source will not be formatted in UTF-8, so if you simply adjust the code like so: pd.read_{filetype}(\"filename.filetype\", encoding: \"latin1\"), it will usually solve the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Three: Data verification\n",
    "\n",
    "In order to verify the data, you need to see it. So the first step here is to call:\n",
    "`df.head`\n",
    "\n",
    "What this will do is deliver you the first five rows of the dataset. After you've seen the dataset for yourself, you can do a number of things:\n",
    "1. use `df.describe()` to see summary statistics of the data\n",
    "1. find unique values using `df['COLUMN NAME'].unique()`\n",
    "1. call `df.columns()` to see the list of columns you're working with\n",
    "\n",
    "This is not an exhaustive list by any means, but these are great tools with which to start understanding your dataset.\n",
    "\n",
    "After this, you can begin dropping any rows or columns you don't need.\n",
    "\n",
    "pandas lets you drop certain null rows using `pd.dropna()`, but be cautious with this--sometimes it makes more sense to keep your null values. An absence of data does not necessarily mean there is an absence of information.\n",
    "\n",
    "Additionally, you can drop rows according to certain conditions. Here is an example:\n",
    "`df = df.drop(df[df['SORT SEQUENCE'] == \"GRAND TOTAL FOR ORGANIZATION\"].index)`\n",
    "\n",
    "Make sure that you're not reassigning variables you'll need later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Four: Visualization\n",
    "\n",
    "There are a few different kinds of modules for visualization purposes, but the most common are matplotlib and seaborn, which is based off of matplotlib. You can import both of them with the following code:\n",
    "\n",
    "`from matplotlib import pyplot as plt\n",
    "import seaborn as sns`\n",
    "\n",
    "After importing the modules, you're pretty much ready to start visualizing!\n",
    "\n",
    "In this guide, we'll be focusing on matplotlib. Here is an example of one of the types of graphs you can make:\n",
    "\n",
    "`plt.hist(df['AMOUNT'], color='blue', edgecolor='black', bins=int(7732336/500000))\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('Amount Paid')\n",
    "plt.ylabel('# of Record')\n",
    "plt.show()`\n",
    "\n",
    "This is a large dataset, so the binning is not quite as standard as it usually would be, but in essence this will give you a histogram. `plt.hist` calls a histogram, while `df['AMOUNT']` calls the column \"AMOUNT\" from the dataset we loaded. You can also set border color and fill color. Finally, you create bins for the histogram.\n",
    "\n",
    "Here, I've used a logarithmic scale to show the full extent of the values without making the columns look too short.\n",
    "\n",
    "You can relabel the X and Y axes--these are sometimes automatically assigned by matplotlib--to more descriptive names.\n",
    "\n",
    "Finally, call `plt.show()` to call a complete graph.\n",
    "\n",
    "Congratulations! You've read data, verified it, and created a visualization!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit308c2c2b353a4a368a60745666fd9a4a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
